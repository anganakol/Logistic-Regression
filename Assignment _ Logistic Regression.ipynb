{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOfWPfIS/zG7ARo2/jOxSzw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"jQmjTOfXNiUj"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["#Theory\n","\n","1. What is Logistic Regression, and how does it differ from Linear Regression?\n","\n","ans - Logistic Regression: A statistical classification method used to predict the probability of an event occurring. It's particularly useful when the outcome variable is binary (e.g., yes/no, pass/fail).\n","Linear Regression: A statistical regression method used to predict a continuous outcome variable. It's suitable for predicting quantities like age, weight, or temperature.\n","Key differences:\n","\n","Output: Logistic Regression predicts the probability of an event, while Linear Regression predicts a continuous value.\n","Loss Function: Logistic Regression uses the log-loss function, while Linear Regression uses the mean squared error.\n","Decision Boundary: Logistic Regression's decision boundary is typically an S-shaped curve (sigmoid function), while Linear Regression's decision boundary is a straight line.\n","\n","2. What is the mathematical equation of Logistic Regression?\n","\n","ans - The logistic regression equation is:\n","\n","$$ log(\\frac{p}{1-p}) = β_0 + β_1x_1 + β_2x_2 + ... + β_kx_k $$\n","\n","3. Why do we use the Sigmoid function in Logistic Regression?\n","\n","ans- The sigmoid function, also known as the logistic function, maps the log odds (the left-hand side of the equation) to a probability between 0 and 1. This makes it suitable for representing the probability of an event.\n","\n","4. What is the cost function of Logistic Regression? Why is it needed?\n","\n","\n","ans- The cost function in logistic regression, also known as the log-loss function, measures how well the model's predictions match the actual outcomes. It's needed to minimize the error between the predicted probabilities and the true labels, thereby optimizing the model's parameters.\n","\n","\n","5. What is Regularization in Logistic Regression? Why is it needed?\n","\n","\n","ans- Regularization is a technique used to prevent overfitting in logistic regression. It adds a penalty term to the cost function, which discourages large coefficient values. This helps to simplify the model and make it more generalizable.\n","\n","\n","6. Explain the difference between Lasso, Ridge, and Elastic Net regression.\n","\n","\n","ans- Lasso (L1 Regularization): Adds the absolute value of the coefficients to the cost function. This can shrink some coefficients to exactly zero, effectively performing feature selection.\n","Ridge (L2 Regularization): Adds the square of the coefficients to the cost function. This encourages smaller coefficients but doesn't force any to become zero.\n","Elastic Net: Combines both L1 and L2 regularization. It can be used to balance between feature selection (like Lasso) and reducing model complexity (like Ridge).\n","\n","7. When should we use Elastic Net instead of Lasso or Ridge?\n","\n","\n","ans- Elastic Net is often preferred when you have a large number of features and expect many of them to be irrelevant. It can help with feature selection and regularization, especially when there's multicollinearity (high correlation) between features.\n","\n","8. What is the impact of the regularization parameter (λ) in Logistic Regression?\n","\n","\n","ans- The regularization parameter (λ) controls the strength of the regularization. A higher λ value leads to more regularization, which shrinks the coefficients towards zero. A lower λ value leads to less regularization, allowing the model to fit the training data more closely.\n","\n","\n","9. What are the key assumptions of Logistic Regression?\n","\n","\n","ans- Binary Outcome: The dependent variable must be binary (two categories).\n","Independence: Observations are assumed to be independent of each other.\n","Linearity in the Log-Odds: The log-odds of the outcome should have a linear relationship with the independent variables.\n","No Multicollinearity: The independent variables should not be highly correlated with each other.\n","\n","10. What are some alternatives to Logistic Regression for classification tasks?\n","\n","\n","ans- Support Vector Machines (SVM): Can handle non-linear decision boundaries and are less affected by outliers.\n","Decision Trees: Can handle categorical variables and non-linear relationships, but can be prone to overfitting.\n","K-Nearest Neighbors (KNN): A simple and intuitive algorithm that can be used for classification.\n","Naive Bayes: A probabilistic classifier that is suitable for handling categorical data.\n","\n","11. What are Classification Evaluation Metrics?\n","\n","\n","ans- Common classification evaluation metrics include:\n","\n","Accuracy: Proportion of correctly classified instances.\n","Precision: Proportion of positive predictions that are correct.\n","Recall: Proportion of true positives that are correctly predicted.\n","F1-Score: Harmonic mean of precision and recall.\n","ROC Curve: Plots the true positive rate against the false positive rate at various thresholds.\n","AUC (Area Under the ROC Curve): Measures the overall ability of the model to rank positive instances higher than negative ones.\n","\n","12. How does class imbalance affect Logistic Regression?\n","\n","\n","ans- Class imbalance refers to a situation where one class has significantly more samples than the other. This can lead to biased models that favor the majority class. To address this, techniques like undersampling, oversampling, or using cost-sensitive learning can be applied.\n","\n","\n","13. What is Hyperparameter Tuning in Logistic Regression?\n","\n","\n","ans- Hyperparameter tuning involves adjusting the parameters of the model (e.g., regularization strength, learning rate) to optimize its performance. This is often done using techniques like grid search or cross-validation.\n","\n","\n","14. What are different solvers in Logistic Regression? Which one should be used?\n","\n","\n","Different solvers are algorithms used to optimize the logistic regression cost function. Common solvers include:\n","\n","Gradient Descent: A simple and widely used method.\n","Newton's Method: Faster than gradient descent but requires more memory.\n","L-BFGS: A quasi-Newton method that combines elements of both gradient descent and Newton's method.\n","\n","15. How is Logistic Regression extended for multiclass classification?\n","\n","\n","ans- Logistic Regression can be extended to handle multiclass classification problems using two main approaches:\n","\n","One-vs-Rest (OvR): Trains one logistic regression model for each class, with that class treated as positive and all others as negative.\n","Softmax Regression: Directly models the probabilities of all classes simultaneously using a softmax function.\n","\n","16. What are the advantages and disadvantages of Logistic Regression?\n","\n","\n","ans- Advantages:\n","\n","Simple and interpretable: The coefficients can be directly interpreted as the log odds ratio.\n","Efficient: Can handle large datasets relatively quickly.\n","Probabilistic output: Provides probabilities for each class, not just a binary prediction.\n","Disadvantages:\n","\n","Assumes linearity: Assumes a linear relationship between the log-odds and the independent variables.\n","Sensitive to outliers: Outliers can heavily influence the model's coefficients.\n","Binary classification only (basic form): Needs to be extended for multiclass classification.\n","\n","17. What are some use cases of Logistic Regression?\n","\n","\n","ans- Logistic Regression is widely used in various fields, including:\n","\n","Marketing: Predicting customer churn, response to promotions, or likelihood of conversion.\n","Finance: Assessing credit risk, predicting loan defaults, or fraud detection.\n","Healthcare: Identifying risk factors for diseases, predicting patient outcomes, or analyzing survival rates.\n","Social Sciences: Studying voting behavior, predicting election outcomes, or analyzing consumer preferences.\n","\n","18. What is the difference between Softmax Regression and Logistic Regression?\n","\n","\n","ans- Softmax Regression is a generalization of Logistic Regression that can be used for multiclass classification problems. Logistic Regression is a special case of Softmax Regression for two classes.\n","\n","19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?\n","\n","\n","ans- The choice between One-vs-Rest and Softmax depends on the specific problem and dataset. One-vs-Rest is simpler to implement and can be more efficient for large datasets. Softmax is generally considered more accurate, especially when the classes are not well-separated.\n","\n","\n","20. How do we interpret coefficients in Logistic Regression?\n","\n","\n","ans- The coefficients in Logistic Regression represent the change in the log odds of the outcome variable for a one-unit change in the corresponding independent variable, holding all other variables constant. The odds ratio can be obtained by exponentiating the coefficient.\n","\n"],"metadata":{"id":"TBZKs9TuNpDL"}},{"cell_type":"markdown","source":["#Practical\n","\n","1. Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy.\n"],"metadata":{"id":"Dqu_3ix_Uqhz"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","from sklearn.datasets import load_iris\n","\n","# Load the iris dataset\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Apply Logistic Regression\n","model = LogisticRegression(random_state=42, solver='liblinear')  # You might need to change the solver\n","model.fit(X_train, y_train)\n","\n","# Make predictions\n","y_pred = model.predict(X_test)\n","\n","# Calculate and print the model accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"Model Accuracy:\", accuracy)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P6Ur-ThVWXYL","executionInfo":{"status":"ok","timestamp":1742823205078,"user_tz":-330,"elapsed":4619,"user":{"displayName":"Aena Majumder","userId":"07644790516573070713"}},"outputId":"5c2f86f2-9369-4aa4-8d89-eea379916d6f"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Model Accuracy: 0.9777777777777777\n"]}]},{"cell_type":"markdown","source":["2. Write a Python\n","program to apply L1 regularization (Lasso) on a dataset\n","using LogisticRegression(penoIty='I1')\n","and print the model accuracy."],"metadata":{"id":"1ujUYDo8WaG7"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","from sklearn.datasets import load_iris\n","\n","# Load the iris dataset\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Apply Logistic Regression with L1 regularization (Lasso)\n","model_lasso = LogisticRegression(penalty='l1', solver='liblinear', random_state=42)\n","model_lasso.fit(X_train, y_train)\n","\n","# Make predictions\n","y_pred_lasso = model_lasso.predict(X_test)\n","\n","# Calculate and print the model accuracy\n","accuracy_lasso = accuracy_score(y_test, y_pred_lasso)\n","print(\"Model Accuracy (Lasso):\", accuracy_lasso)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KzcO5jVjWzM_","executionInfo":{"status":"ok","timestamp":1742823213173,"user_tz":-330,"elapsed":3,"user":{"displayName":"Aena Majumder","userId":"07644790516573070713"}},"outputId":"c25f913c-7b2a-40e2-cc45-b28431891cf2"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Model Accuracy (Lasso): 1.0\n"]}]},{"cell_type":"markdown","source":["3. Write a Python\n","program to train Logistic Regression with L2 regularization (Ridge) using\n","LogisticRegression(penaIty=’I2’). Print model accuracy and coefficients."],"metadata":{"id":"IDOv7mKCW_QF"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","from sklearn.datasets import load_iris\n","\n","# Load the iris dataset\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Apply Logistic Regression with L2 regularization (Ridge)\n","model_ridge = LogisticRegression(penalty='l2', solver='liblinear', random_state=42)\n","model_ridge.fit(X_train, y_train)\n","\n","# Make predictions\n","y_pred_ridge = model_ridge.predict(X_test)\n","\n","# Calculate and print the model accuracy\n","accuracy_ridge = accuracy_score(y_test, y_pred_ridge)\n","print(\"Model Accuracy (Ridge):\", accuracy_ridge)\n","\n","# Print the coefficients\n","print(\"Coefficients:\", model_ridge.coef_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M_ZLsvJtXJ-d","executionInfo":{"status":"ok","timestamp":1742823275121,"user_tz":-330,"elapsed":454,"user":{"displayName":"Aena Majumder","userId":"07644790516573070713"}},"outputId":"bcc6ca28-fc8e-4cd4-847d-1e2e8a788deb"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Model Accuracy (Ridge): 0.9777777777777777\n","Coefficients: [[ 0.36479402  1.35499766 -2.09628559 -0.92154751]\n"," [ 0.4808915  -1.58463288  0.3937527  -1.09224057]\n"," [-1.5286415  -1.43244729  2.3048277   2.08584535]]\n"]}]},{"cell_type":"markdown","source":["4. Write a Python program to train Logistic Regression with Elastic Net Regularization (penaIty='eIasticnet')."],"metadata":{"id":"DEkmooDBXMSr"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","from sklearn.datasets import load_iris\n","\n","# Load the iris dataset\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Apply Logistic Regression with Elastic Net regularization\n","# Elastic Net requires both 'l1_ratio' and a solver that supports it (e.g., 'saga', 'elasticnet')\n","model_enet = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, random_state=42, max_iter=1000)\n","model_enet.fit(X_train, y_train)\n","\n","# Make predictions\n","y_pred_enet = model_enet.predict(X_test)\n","\n","# Calculate and print the model accuracy\n","accuracy_enet = accuracy_score(y_test, y_pred_enet)\n","print(\"Model Accuracy (Elastic Net):\", accuracy_enet)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oc5hxaGnXaE1","executionInfo":{"status":"ok","timestamp":1742823340287,"user_tz":-330,"elapsed":498,"user":{"displayName":"Aena Majumder","userId":"07644790516573070713"}},"outputId":"b4fe369c-9836-467c-e629-1ca1e120b027"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Model Accuracy (Elastic Net): 1.0\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","source":["5. Write a Python program to train a Logistic Regression model for multiclass classification using\n","\n","multi_cIass='ovr'."],"metadata":{"id":"C5x-f7MtXcYg"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","from sklearn.datasets import load_iris\n","\n","# Load the iris dataset\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Apply Logistic Regression with multi_class='ovr'\n","model_ovr = LogisticRegression(multi_class='ovr', solver='liblinear', random_state=42)\n","model_ovr.fit(X_train, y_train)\n","\n","# Make predictions\n","y_pred_ovr = model_ovr.predict(X_test)\n","\n","# Calculate and print the model accuracy\n","accuracy_ovr = accuracy_score(y_test, y_pred_ovr)\n","print(\"Model Accuracy (OvR):\", accuracy_ovr)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nY1ZdSQKXkIg","executionInfo":{"status":"ok","timestamp":1742823385261,"user_tz":-330,"elapsed":434,"user":{"displayName":"Aena Majumder","userId":"07644790516573070713"}},"outputId":"3e722575-8707-439f-bcf7-25c412ccdc0b"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Model Accuracy (OvR): 0.9777777777777777\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","source":["6. Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy."],"metadata":{"id":"UCFTMxYwXnXn"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","from sklearn.datasets import load_iris\n","\n","# Load the iris dataset\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Define the parameter grid for hyperparameter tuning\n","# C: Inverse of regularization strength\n","# penalty: Type of regularization ('l1' or 'l2')\n","param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n","              'penalty': ['l1', 'l2']}\n","\n","# Create a Logistic Regression model\n","model = LogisticRegression(solver='liblinear', random_state=42)  # 'liblinear' is suitable for l1/l2\n","\n","# Use GridSearchCV to find the best hyperparameters\n","grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')  # 5-fold cross-validation\n","grid_search.fit(X_train, y_train)\n","\n","# Get the best parameters and best accuracy\n","best_params = grid_search.best_params_\n","best_accuracy = grid_search.best_score_  # Accuracy from cross-validation\n","\n","# Print the best parameters and accuracy\n","print(\"Best Parameters:\", best_params)\n","print(\"Best Accuracy (Cross-Validation):\", best_accuracy)\n","\n","# Evaluate the model on the test set (optional, but recommended)\n","best_model = grid_search.best_estimator_\n","y_pred_test = best_model.predict(X_test)\n","test_accuracy = accuracy_score(y_test, y_pred_test)\n","print(\"Test Set Accuracy:\", test_accuracy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lmVlsdXsX8sU","executionInfo":{"status":"ok","timestamp":1742823499728,"user_tz":-330,"elapsed":708,"user":{"displayName":"Aena Majumder","userId":"07644790516573070713"}},"outputId":"436550e5-9c0f-4710-9cd5-a814515adee4"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Best Parameters: {'C': 10, 'penalty': 'l2'}\n","Best Accuracy (Cross-Validation): 0.9523809523809523\n","Test Set Accuracy: 1.0\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","source":["7. Write a Python\n","\n","program to evaluate Logistic\n","\n","Regression using Stratified K-Fold Cross-Validation. Print the average accuracy."],"metadata":{"id":"CcwygCmlYD1J"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import StratifiedKFold, cross_val_score\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.datasets import load_iris\n","\n","# Load the iris dataset\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Create StratifiedKFold cross-validation object\n","cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)  # You can adjust n_splits\n","\n","# Create a Logistic Regression model\n","model = LogisticRegression(solver='liblinear', random_state=42)  # Choose an appropriate solver\n","\n","# Perform Stratified K-Fold Cross-Validation\n","cv_scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n","\n","# Calculate the average accuracy\n","average_accuracy = cv_scores.mean()\n","\n","# Print the average accuracy\n","print(\"Average Accuracy (Stratified K-Fold):\", average_accuracy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ep11pMPmYKc6","executionInfo":{"status":"ok","timestamp":1742823563920,"user_tz":-330,"elapsed":437,"user":{"displayName":"Aena Majumder","userId":"07644790516573070713"}},"outputId":"7e1052d5-cfa1-4ce1-a24f-50cae4af6d3e"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Average Accuracy (Stratified K-Fold): 0.96\n"]}]},{"cell_type":"markdown","source":["8. Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its\n","accuracy."],"metadata":{"id":"W1-3FJW3YTtd"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","\n","# Load the dataset from the CSV file\n","# Replace 'your_dataset.csv' with the actual path to your CSV file\n","data = pd.read_csv('your_dataset.csv')\n","\n","# Separate features (X) and target variable (y)\n","# Adjust the column names as per your CSV file\n","X = data.drop('target_column_name', axis=1)  # Replace 'target_column_name'\n","y = data['target_column_name']\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Apply Logistic Regression\n","model = LogisticRegression(solver='liblinear')  # Choose an appropriate solver\n","\n","# Train the model\n","model.fit(X_train, y_train)\n","\n","# Make predictions on the test set\n","y_pred = model.predict(X_test)\n","\n","# Evaluate the model's accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","\n","# Print the accuracy\n","print(\"Model Accuracy:\", accuracy)"],"metadata":{"id":"2OOs_scQYm2x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["9.\tWrite a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy"],"metadata":{"id":"iGGub2LcYnxj"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split, RandomizedSearchCV\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","from sklearn.datasets import load_iris\n","import numpy as np  # For defining parameter distributions\n","\n","# Load the iris dataset\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Define the parameter distributions for hyperparameter tuning\n","# C: Inverse of regularization strength (log-uniform distribution)\n","# penalty: Type of regularization ('l1', 'l2', or 'elasticnet')\n","# solver: Algorithm to use in the optimization problem\n","param_distributions = {\n","    'C': np.logspace(-4, 4, 20),  # Try values from 0.0001 to 10000\n","    'penalty': ['l1', 'l2', 'elasticnet'],\n","    'solver': ['liblinear', 'saga']  # Solvers that handle different penalties\n","}\n","\n","# Create a Logistic Regression model\n","model = LogisticRegression(random_state=42, max_iter=1000)  # Increased max_iter for convergence\n","\n","# Use RandomizedSearchCV to find the best hyperparameters\n","random_search = RandomizedSearchCV(model,\n","                                   param_distributions,\n","                                   n_iter=20,  # Number of parameter settings sampled\n","                                   cv=5,       # 5-fold cross-validation\n","                                   scoring='accuracy',\n","                                   random_state=42)\n","random_search.fit(X_train, y_train)\n","\n","# Get the best parameters and best accuracy\n","best_params = random_search.best_params_\n","best_accuracy = random_search.best_score_  # Accuracy from cross-validation\n","\n","# Print the best parameters and accuracy\n","print(\"Best Parameters:\", best_params)\n","print(\"Best Accuracy (Cross-Validation):\", best_accuracy)\n","\n","# Evaluate the model on the test set (optional, but recommended)\n","best_model = random_search.best_estimator_\n","y_pred_test = best_model.predict(X_test)\n","test_accuracy = accuracy_score(y_test, y_pred_test)\n","print(\"Test Set Accuracy:\", test_accuracy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"01RJ-0B-Y6cg","executionInfo":{"status":"ok","timestamp":1742823760880,"user_tz":-330,"elapsed":1462,"user":{"displayName":"Aena Majumder","userId":"07644790516573070713"}},"outputId":"ce7b49f0-4634-444a-ab9c-de077a414b8e"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Best Parameters: {'solver': 'saga', 'penalty': 'l1', 'C': np.float64(0.615848211066026)}\n","Best Accuracy (Cross-Validation): 0.9619047619047618\n","Test Set Accuracy: 1.0\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n","45 fits failed out of a total of 100.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","20 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n","    return fit_method(estimator, *args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1203, in fit\n","    raise ValueError(\"l1_ratio must be specified when penalty is elasticnet.\")\n","ValueError: l1_ratio must be specified when penalty is elasticnet.\n","\n","--------------------------------------------------------------------------------\n","25 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n","    return fit_method(estimator, *args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 71, in _check_solver\n","    raise ValueError(\n","ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.78095238        nan        nan 0.96190476 0.64761905        nan\n"," 0.96190476        nan        nan        nan 0.2952381  0.95238095\n","        nan 0.35238095        nan 0.96190476 0.96190476 0.2952381\n","        nan 0.94285714]\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","source":["10.\tWrite a Python program to implement One—vs—One (OvO) Multiclass Logistic Regression and print accuracy."],"metadata":{"id":"Gx2IRk7ZZEg0"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.multiclass import OneVsOneClassifier\n","from sklearn.metrics import accuracy_score\n","from sklearn.datasets import load_iris\n","\n","# Load the iris dataset\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Define the base Logistic Regression model\n","logistic_model = LogisticRegression(solver='liblinear', random_state=42)  # Choose an appropriate solver\n","\n","# Implement One-vs-One Multiclass Classification\n","ovo_model = OneVsOneClassifier(logistic_model)\n","\n","# Train the OvO model\n","ovo_model.fit(X_train, y_train)\n","\n","# Make predictions\n","y_pred_ovo = ovo_model.predict(X_test)\n","\n","# Calculate and print the model accuracy\n","accuracy_ovo = accuracy_score(y_test, y_pred_ovo)\n","print(\"Model Accuracy (One-vs-One):\", accuracy_ovo)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rTjDXwzoZIvY","executionInfo":{"status":"ok","timestamp":1742823812955,"user_tz":-330,"elapsed":416,"user":{"displayName":"Aena Majumder","userId":"07644790516573070713"}},"outputId":"bf9bd166-83dc-435a-da01-f8a9e0ed1082"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Model Accuracy (One-vs-One): 1.0\n"]}]},{"cell_type":"markdown","source":["11.\tWrite a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification."],"metadata":{"id":"6L_Eo56pZQUI"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import confusion_matrix\n","import seaborn as sns\n","from sklearn.datasets import load_iris  # Or any other binary classification dataset\n","\n","# Load a dataset (using iris for demonstration, but we'll make it binary)\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# For binary classification, let's use the first two classes (0 and 1)\n","X = X[y < 2]\n","y = y[y < 2]\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Apply Logistic Regression\n","model = LogisticRegression(solver='liblinear')  # 'liblinear' is suitable for binary classification\n","model.fit(X_train, y_train)\n","\n","# Make predictions on the test set\n","y_pred = model.predict(X_test)\n","\n","# Generate the confusion matrix\n","cm = confusion_matrix(y_test, y_pred)\n","\n","# Visualize the confusion matrix\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n","plt.xlabel('Predicted labels')\n","plt.ylabel('True labels')\n","plt.title('Confusion Matrix for Logistic Regression')\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":564},"id":"MFXao9MWZTNr","executionInfo":{"status":"ok","timestamp":1742823864201,"user_tz":-330,"elapsed":3466,"user":{"displayName":"Aena Majumder","userId":"07644790516573070713"}},"outputId":"8dd7a76e-f5a0-43d9-bcd7-17f810ca2d0c"},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 800x600 with 2 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":["12.\tWrite a Python program to train a Logistic Regression model and evaluate its performance using Precision, Recall, and F1-Score."],"metadata":{"id":"7uy3z4oHZcSF"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","from sklearn.datasets import load_iris  # Or any other dataset\n","\n","# Load a dataset (using iris for demonstration)\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# For binary classification, let's use the first two classes (0 and 1)\n","X = X[y < 2]\n","y = y[y < 2]\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Apply Logistic Regression\n","model = LogisticRegression(solver='liblinear')  # Choose an appropriate solver\n","model.fit(X_train, y_train)\n","\n","# Make predictions on the test set\n","y_pred = model.predict(X_test)\n","\n","# Calculate Precision, Recall, and F1-Score\n","precision = precision_score(y_test, y_pred)\n","recall = recall_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","\n","# Print the evaluation metrics\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"F1-Score:\", f1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s_qgnh3PZeU0","executionInfo":{"status":"ok","timestamp":1742823945464,"user_tz":-330,"elapsed":440,"user":{"displayName":"Aena Majumder","userId":"07644790516573070713"}},"outputId":"3a3e2ed3-08d1-41c3-89e0-7237647b2185"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Precision: 1.0\n","Recall: 1.0\n","F1-Score: 1.0\n"]}]},{"cell_type":"markdown","source":["13.\tWrite a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance."],"metadata":{"id":"6DLqJxDvZwNt"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import classification_report\n","from sklearn.datasets import load_iris  # Or any other dataset\n","\n","# Load a dataset (using iris for demonstration, but we'll make it imbalanced)\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Create an imbalanced dataset (for demonstration)\n","# Let's say class 0 is the minority class, and classes 1 and 2 are the majority\n","X = X[y != 0]\n","y = y[y != 0]\n","y[y == 1] = 0  # Map class 1 to 0\n","y[y == 2] = 1  # Map class 2 to 1\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Apply Logistic Regression WITHOUT class weights (for baseline)\n","model_no_weights = LogisticRegression(solver='liblinear')\n","model_no_weights.fit(X_train, y_train)\n","y_pred_no_weights = model_no_weights.predict(X_test)\n","\n","print(\"Classification Report (No Class Weights):\")\n","print(classification_report(y_test, y_pred_no_weights))\n","\n","# Apply Logistic Regression WITH class weights\n","model_with_weights = LogisticRegression(solver='liblinear', class_weight='balanced')\n","model_with_weights.fit(X_train, y_train)\n","y_pred_with_weights = model_with_weights.predict(X_test)\n","\n","print(\"\\nClassification Report (With Class Weights):\")\n","print(classification_report(y_test, y_pred_with_weights))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SZ2V83tNZyJS","executionInfo":{"status":"ok","timestamp":1742823990045,"user_tz":-330,"elapsed":433,"user":{"displayName":"Aena Majumder","userId":"07644790516573070713"}},"outputId":"0ffb8b0e-ae10-42d4-9327-b6d05931ee5d"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Classification Report (No Class Weights):\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.82      0.90        17\n","           1       0.81      1.00      0.90        13\n","\n","    accuracy                           0.90        30\n","   macro avg       0.91      0.91      0.90        30\n","weighted avg       0.92      0.90      0.90        30\n","\n","\n","Classification Report (With Class Weights):\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.88      0.94        17\n","           1       0.87      1.00      0.93        13\n","\n","    accuracy                           0.93        30\n","   macro avg       0.93      0.94      0.93        30\n","weighted avg       0.94      0.93      0.93        30\n","\n"]}]},{"cell_type":"markdown","source":["14.\tWrite a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performance."],"metadata":{"id":"o2-v3n7XZ7VO"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, classification_report\n","from sklearn.impute import SimpleImputer  # For handling missing values\n","\n","# Load the Titanic dataset\n","# You might need to download the dataset (e.g., from Kaggle) and adjust the path\n","data = pd.read_csv('titanic.csv')\n","\n","# Data Preprocessing\n","# 1. Handle Missing Values\n","#    - Impute 'Age' with the median\n","#    - Impute 'Embarked' with the most frequent value\n","imputer_age = SimpleImputer(strategy='median')\n","data['Age'] = imputer_age.fit_transform(data[['Age']])\n","\n","imputer_embarked = SimpleImputer(strategy='most_frequent')\n","data['Embarked'] = imputer_embarked.fit_transform(data[['Embarked']])\n","\n","# 2. Drop unnecessary columns (or those with too many missing values if not imputing)\n","data = data.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)\n","\n","# 3. Convert categorical variables into numerical\n","data['Sex'] = data['Sex'].map({'male': 0, 'female': 1})\n","data['Embarked'] = data['Embarked'].map({'S': 0, 'C': 1, 'Q': 2})\n","\n","# Separate features (X) and target variable (y)\n","X = data.drop('Survived', axis=1)\n","y = data['Survived']\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Apply Logistic Regression\n","model = LogisticRegression(solver='liblinear')  # Choose an appropriate solver\n","model.fit(X_train, y_train)\n","\n","# Make predictions on the test set\n","y_pred = model.predict(X_test)\n","\n","# Evaluate the model's performance\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"Accuracy:\", accuracy)\n","\n","# Print classification report for more detailed metrics\n","print(\"\\nClassification Report:\")\n","print(classification_report(y_test, y_pred))"],"metadata":{"id":"7a2VhFmraIux"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["15.\tWrite a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scaling."],"metadata":{"id":"AXYSQcGGaKKN"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import accuracy_score\n","\n","# Load the iris dataset (or your dataset)\n","from sklearn.datasets import load_iris\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# 1. Logistic Regression WITHOUT Scaling\n","model_no_scaling = LogisticRegression(solver='liblinear')\n","model_no_scaling.fit(X_train, y_train)\n","y_pred_no_scaling = model_no_scaling.predict(X_test)\n","accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n","\n","print(\"Accuracy without Scaling:\", accuracy_no_scaling)\n","\n","# 2. Logistic Regression WITH Standardization (Feature Scaling)\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)  # Scale training data\n","X_test_scaled = scaler.transform(X_test)        # Scale test data using the same scaler\n","\n","model_scaled = LogisticRegression(solver='liblinear')\n","model_scaled.fit(X_train_scaled, y_train)\n","y_pred_scaled = model_scaled.predict(X_test_scaled)\n","accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n","\n","print(\"Accuracy with Standardization:\", accuracy_scaled)\n","\n","# Comparison\n","print(\"\\nComparison:\")\n","if accuracy_scaled > accuracy_no_scaling:\n","    print(\"Standardization improved accuracy.\")\n","elif accuracy_scaled < accuracy_no_scaling:\n","    print(\"Accuracy decreased with Standardization.\")\n","else:\n","    print(\"Standardization did not change accuracy.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LTqJAAi4Jw-I","executionInfo":{"status":"ok","timestamp":1742836670052,"user_tz":-330,"elapsed":4995,"user":{"displayName":"Aena Majumder","userId":"07644790516573070713"}},"outputId":"980d41e3-2386-4747-f6c4-3b6a6613796a"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy without Scaling: 0.9777777777777777\n","Accuracy with Standardization: 0.9111111111111111\n","\n","Comparison:\n","Accuracy decreased with Standardization.\n"]}]},{"cell_type":"markdown","source":["16.\tWrite a Python program to train Logistic Regression and evaluate its performance using ROC—AUC score."],"metadata":{"id":"fRwwIsCHKS8S"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import roc_auc_score\n","from sklearn.datasets import load_iris  # Or any other dataset\n","\n","# Load a dataset (using iris for demonstration, but we'll make it binary)\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# For binary classification, let's use the first two classes (0 and 1)\n","X = X[y < 2]\n","y = y[y < 2]\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Apply Logistic Regression\n","model = LogisticRegression(solver='liblinear')  # Choose an appropriate solver\n","model.fit(X_train, y_train)\n","\n","# Make predictions on the test set\n","# For ROC AUC, we need the *probabilities* of the predictions, not the class labels directly\n","y_prob = model.predict_proba(X_test)[:, 1]  # Probabilities for the positive class (class 1)\n","\n","# Calculate ROC AUC Score\n","roc_auc = roc_auc_score(y_test, y_prob)\n","\n","# Print the ROC AUC score\n","print(\"ROC AUC Score:\", roc_auc)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WDPdahrWKVW-","executionInfo":{"status":"ok","timestamp":1742836725442,"user_tz":-330,"elapsed":741,"user":{"displayName":"Aena Majumder","userId":"07644790516573070713"}},"outputId":"c52213d8-710d-4617-edaf-96e3ecb59b04"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["ROC AUC Score: 1.0\n"]}]},{"cell_type":"markdown","source":["17.\tWrite a Python program to train Logistic Regression using a custom learning rote (C=0.5) and evaluate accuracy"],"metadata":{"id":"fPvHjR7GKgW4"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","from sklearn.datasets import load_iris  # Or any other dataset\n","\n","# Load a dataset (using iris for demonstration)\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Apply Logistic Regression with C=0.5\n","# 'C' is the inverse of regularization strength. Smaller values specify stronger regularization.\n","model = LogisticRegression(solver='liblinear', C=0.5)  # Choose an appropriate solver\n","model.fit(X_train, y_train)\n","\n","# Make predictions on the test set\n","y_pred = model.predict(X_test)\n","\n","# Evaluate the model's performance\n","accuracy = accuracy_score(y_test, y_pred)\n","\n","# Print the accuracy\n","print(\"Accuracy with C=0.5:\", accuracy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DQ1dZJzjKi5b","executionInfo":{"status":"ok","timestamp":1742837824583,"user_tz":-330,"elapsed":420,"user":{"displayName":"Aena Majumder","userId":"07644790516573070713"}},"outputId":"4e4f1914-a9f3-4188-dd71-2309bdd14816"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy with C=0.5: 0.9555555555555556\n"]}]},{"cell_type":"markdown","source":["18.\tWrite a Python program to train Logistic Regression and identify important features based on model coefficients."],"metadata":{"id":"2tKa9UUUOusy"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.datasets import load_iris  # Or any other dataset\n","\n","# Load a dataset (using iris for demonstration)\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Apply Logistic Regression\n","model = LogisticRegression(solver='liblinear')  # Choose an appropriate solver\n","model.fit(X_train, y_train)\n","\n","# Get the coefficients\n","coefficients = model.coef_[0]  # For binary classification, it's model.coef_[0]\n","\n","# Get feature names (if available)\n","feature_names = iris.feature_names  # ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n","\n","# Create a DataFrame to display feature importance\n","feature_importance = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients})\n","feature_importance['AbsoluteCoefficient'] = feature_importance['Coefficient'].abs()  # Get absolute values\n","feature_importance = feature_importance.sort_values(by='AbsoluteCoefficient', ascending=False)  # Sort by importance\n","\n","# Print the feature importance\n","print(\"Feature Importance:\")\n","print(feature_importance)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z1hJ2EYQPX1Q","executionInfo":{"status":"ok","timestamp":1742838009282,"user_tz":-330,"elapsed":694,"user":{"displayName":"Aena Majumder","userId":"07644790516573070713"}},"outputId":"2e3679be-2b57-4360-da70-0e3718dd683b"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Feature Importance:\n","             Feature  Coefficient  AbsoluteCoefficient\n","2  petal length (cm)    -2.096286             2.096286\n","1   sepal width (cm)     1.354998             1.354998\n","3   petal width (cm)    -0.921548             0.921548\n","0  sepal length (cm)     0.364794             0.364794\n"]}]},{"cell_type":"markdown","source":["19.\tWrite a Python program to train Logistic Regression and evaluate its performance using Cohen's Kappa Score."],"metadata":{"id":"jmgZ3EjBPZuA"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import cohen_kappa_score\n","from sklearn.datasets import load_iris  # Or any other dataset\n","\n","# Load a dataset (using iris for demonstration)\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Apply Logistic Regression\n","model = LogisticRegression(solver='liblinear')  # Choose an appropriate solver\n","model.fit(X_train, y_train)\n","\n","# Make predictions on the test set\n","y_pred = model.predict(X_test)\n","\n","# Evaluate the model's performance using Cohen's Kappa Score\n","kappa = cohen_kappa_score(y_test, y_pred)\n","\n","# Print the Cohen's Kappa Score\n","print(\"Cohen's Kappa Score:\", kappa)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qh8aunS4Pi0u","executionInfo":{"status":"ok","timestamp":1742838055759,"user_tz":-330,"elapsed":445,"user":{"displayName":"Aena Majumder","userId":"07644790516573070713"}},"outputId":"c15acf86-1633-4272-ad84-18a0ff86202c"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Cohen's Kappa Score: 0.9660633484162896\n"]}]},{"cell_type":"markdown","source":["20.\tWrite a Python program to train Logistic Regression and visualize the Precision— Recall Curve for binary classification"],"metadata":{"id":"TnAsVlcKPlSh"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import precision_recall_curve\n","from sklearn.datasets import load_iris  # Or any other dataset\n","import matplotlib.pyplot as plt\n","\n","# Load a dataset (using iris for demonstration, but we'll make it binary)\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# For binary classification, let's use the first two classes (0 and 1)\n","X = X[y < 2]\n","y = y[y < 2]\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Apply Logistic Regression\n","model = LogisticRegression(solver='liblinear')  # Choose an appropriate solver\n","model.fit(X_train, y_train)\n","\n","# Get predicted probabilities for the positive class\n","y_prob = model.predict_proba(X_test)[:, 1]\n","\n","# Calculate Precision-Recall curve\n","precision, recall, thresholds = precision_recall_curve(y_test, y_prob)\n","\n","# Plot the Precision-Recall curve\n","plt.figure(figsize=(8, 6))\n","plt.plot(recall, precision, color='darkorange', lw=2)\n","plt.xlim()\n","plt.ylim()\n","plt.xlabel('Recall')\n","plt.ylabel('Precision')\n","plt.title('Precision-Recall Curve')\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":564},"id":"ypmCQsfYPv6o","executionInfo":{"status":"ok","timestamp":1742838107665,"user_tz":-330,"elapsed":1485,"user":{"displayName":"Aena Majumder","userId":"07644790516573070713"}},"outputId":"3c8113bd-a3e6-4ddb-c5b7-9f108a7b2237"},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 800x600 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":["21.\tWrite a Python program to train Logistic Regression with different solvers (liblinear, saga, Ibfgs) and compare their accuracy."],"metadata":{"id":"3aGkBxSnPylg"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","from sklearn.datasets import load_iris  # Or any other dataset\n","\n","# Load a dataset (using iris for demonstration)\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Solvers to compare\n","solvers = ['liblinear', 'saga', 'lbfgs']\n","accuracies = {}\n","\n","# Train Logistic Regression with each solver and evaluate\n","for solver in solvers:\n","    model = LogisticRegression(solver=solver, max_iter=10000)  # Increased max_iter for convergence\n","    model.fit(X_train, y_train)\n","    y_pred = model.predict(X_test)\n","    accuracy = accuracy_score(y_test, y_pred)\n","    accuracies[solver] = accuracy\n","    print(f\"Solver: {solver}, Accuracy: {accuracy}\")\n","\n","# Compare accuracies\n","best_solver = max(accuracies, key=accuracies.get)\n","print(f\"\\nBest Solver: {best_solver} with Accuracy: {accuracies[best_solver]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RcQ_AHiVP1Fo","executionInfo":{"status":"ok","timestamp":1742838143684,"user_tz":-330,"elapsed":446,"user":{"displayName":"Aena Majumder","userId":"07644790516573070713"}},"outputId":"e5bfb09d-ed46-4a08-caff-0852718e9ce0"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Solver: liblinear, Accuracy: 0.9777777777777777\n","Solver: saga, Accuracy: 1.0\n","Solver: lbfgs, Accuracy: 1.0\n","\n","Best Solver: saga with Accuracy: 1.0\n"]}]},{"cell_type":"markdown","source":["22.\tWrite a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC)."],"metadata":{"id":"3Tn1ffNgP6X7"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import matthews_corrcoef\n","from sklearn.datasets import load_iris  # Or any other dataset\n","\n","# Load a dataset (using iris for demonstration, but we'll make it binary)\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# For binary classification, let's use the first two classes (0 and 1)\n","X = X[y < 2]\n","y = y[y < 2]\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Apply Logistic Regression\n","model = LogisticRegression(solver='liblinear')  # Choose an appropriate solver\n","model.fit(X_train, y_train)\n","\n","# Make predictions on the test set\n","y_pred = model.predict(X_test)\n","\n","# Evaluate the model's performance using Matthews Correlation Coefficient (MCC)\n","mcc = matthews_corrcoef(y_test, y_pred)\n","\n","# Print the Matthews Correlation Coefficient (MCC)\n","print(\"Matthews Correlation Coefficient (MCC):\", mcc)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FdFxxnrGQBZk","executionInfo":{"status":"ok","timestamp":1742838178846,"user_tz":-330,"elapsed":576,"user":{"displayName":"Aena Majumder","userId":"07644790516573070713"}},"outputId":"9ba39255-927e-43e0-edef-36d919ab6a6c"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Matthews Correlation Coefficient (MCC): 1.0\n"]}]},{"cell_type":"markdown","source":["23.\tWrite a Python program to train Logistic Regression on both row and standardized data. Compare their accuracy to see the impact of feature scaling."],"metadata":{"id":"VPELvD0ZQC4p"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import accuracy_score\n","from sklearn.datasets import load_iris  # Or any other dataset\n","\n","# Load a dataset (using iris for demonstration)\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# 1. Logistic Regression on Raw Data\n","model_raw = LogisticRegression(solver='liblinear')\n","model_raw.fit(X_train, y_train)\n","y_pred_raw = model_raw.predict(X_test)\n","accuracy_raw = accuracy_score(y_test, y_pred_raw)\n","\n","print(\"Accuracy on Raw Data:\", accuracy_raw)\n","\n","# 2. Logistic Regression on Standardized Data\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","model_scaled = LogisticRegression(solver='liblinear')\n","model_scaled.fit(X_train_scaled, y_train)\n","y_pred_scaled = model_scaled.predict(X_test_scaled)\n","accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n","\n","print(\"Accuracy on Standardized Data:\", accuracy_scaled)\n","\n","# Comparison\n","print(\"\\nComparison:\")\n","if accuracy_scaled > accuracy_raw:\n","    print(\"Standardization improved accuracy.\")\n","elif accuracy_scaled < accuracy_raw:\n","    print(\"Raw data performed better.\")\n","else:\n","    print(\"Standardization did not change accuracy.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FrlkJt3cQHIl","executionInfo":{"status":"ok","timestamp":1742838329254,"user_tz":-330,"elapsed":456,"user":{"displayName":"Aena Majumder","userId":"07644790516573070713"}},"outputId":"9086045b-f589-4dc2-9c82-48e7b810661b"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy on Raw Data: 0.9777777777777777\n","Accuracy on Standardized Data: 0.9111111111111111\n","\n","Comparison:\n","Raw data performed better.\n"]}]},{"cell_type":"markdown","source":["24.\tWrite a Python program to train Logistic Regression and find the optimal C (regulorizotion strength) using cross—validation."],"metadata":{"id":"cvuwTwJKQoBu"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split, cross_val_score, KFold\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.datasets import load_iris  # Or any other dataset\n","import numpy as np\n","\n","# Load a dataset (using iris for demonstration)\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Split the dataset into training and testing sets (for initial split)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Standardize the data\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# Define the range of C values to test\n","C_values = [0.001, 0.01, 0.1, 1, 10, 100]\n","\n","# Perform cross-validation to find the optimal C\n","def find_optimal_c(X_train, y_train, C_values, cv=5):\n","    \"\"\"\n","    Finds the optimal C value for Logistic Regression using cross-validation.\n","\n","    Args:\n","        X_train: Training features.\n","        y_train: Training target.\n","        C_values: List of C values to test.\n","        cv: Number of cross-validation folds.\n","\n","    Returns:\n","        The optimal C value and the corresponding mean cross-validation score.\n","    \"\"\"\n","\n","    mean_cv_scores =\n","\n","    for C in C_values:\n","        model = LogisticRegression(solver='liblinear', C=C, max_iter=10000)\n","        cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')  # Using accuracy as scoring\n","        mean_cv_scores.append(np.mean(cv_scores))\n","\n","    optimal_C = C_values[np.argmax(mean_cv_scores)]\n","    optimal_score = np.max(mean_cv_scores)\n","\n","    return optimal_C, optimal_score\n","\n","optimal_C, optimal_score = find_optimal_c(X_train_scaled, y_train, C_values)\n","\n","print(\"Optimal C:\", optimal_C)\n","print(\"Optimal Cross-Validation Score:\", optimal_score)\n","\n","# Train the final model with the optimal C on the entire training set\n","final_model = LogisticRegression(solver='liblinear', C=optimal_C, max_iter=10000)\n","final_model.fit(X_train_scaled, y_train)\n","\n","# Evaluate the final model on the test set\n","y_pred_final = final_model.predict(X_test_scaled)\n","accuracy_final = accuracy_score(y_test, y_pred_final)\n","print(\"Test Accuracy with Optimal C:\", accuracy_final)"],"metadata":{"id":"wrZZRB4KQxZI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["25.\tWrite a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions."],"metadata":{"id":"gdlZ1JpPQyML"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.datasets import load_iris  # Or any other dataset\n","from sklearn.preprocessing import StandardScaler\n","import joblib  # For saving and loading the model\n","\n","# Load a dataset (using iris for demonstration)\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Standardize the data (good practice before Logistic Regression)\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# 1. Train Logistic Regression\n","model = LogisticRegression(solver='liblinear')  # Choose a suitable solver\n","model.fit(X_train_scaled, y_train)\n","\n","# 2. Save the trained model\n","filename = 'logistic_regression_model.joblib'  # Choose a filename\n","joblib.dump(model, filename)\n","print(f\"Trained model saved as {filename}\")\n","\n","# 3. Load the model and make predictions\n","loaded_model = joblib.load(filename)\n","print(f\"Model loaded from {filename}\")\n","\n","# Make predictions using the loaded model\n","y_pred = loaded_model.predict(X_test_scaled)\n","\n","# Print the predictions (or evaluate them as needed)\n","print(\"Predictions using loaded model:\", y_pred)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G910JKI6Q6bN","executionInfo":{"status":"ok","timestamp":1742838412509,"user_tz":-330,"elapsed":418,"user":{"displayName":"Aena Majumder","userId":"07644790516573070713"}},"outputId":"5a5fcf54-5912-4a6a-b35e-bd42a516f542"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Trained model saved as logistic_regression_model.joblib\n","Model loaded from logistic_regression_model.joblib\n","Predictions using loaded model: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 2 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 2\n"," 0 0 0 2 2 2 0 0]\n"]}]}]}